{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE-MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrnbvOcvn44w",
        "colab_type": "text"
      },
      "source": [
        "This is a Google Colab notebook which installs appropriate PyTorch v1 according to the system architecture and the GPU available.\n",
        "\n",
        "If you come across any issues, then feel free to contact me on Slack. My username is ***avinashss***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtN00cb0n6hZ",
        "colab_type": "code",
        "outputId": "e37ccacc-496a-4844-cc03-6ef98b19ff82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# google colab does not come with torch installed. And also, in lesson we are \n",
        "# using torch v1.0 \n",
        "# so following snippet of code installs the relevant version according to the \n",
        "# GPU architecture\n",
        "!pip install -q torch==1.1.0 torchvision\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9T-evMrqds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77034c4b-e517-4d65-aa04-451f24aca26e"
      },
      "source": [
        "# we will verify that GPU is enabled for this notebook\n",
        "# following should print: CUDA is available!  Training on GPU ...\n",
        "# \n",
        "# if it prints otherwise, then you need to enable GPU: \n",
        "# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKQGml-R_jxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ef9983c1-6070-44fe-e48a-0a77cae1a876"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 16 01:51:37 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    16W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA32GQtL_sS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, argparse\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch, os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4AroTSe_ymC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(recon_x, x):\n",
        "    return F.mse_loss(recon_x, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VsgYfMLAFYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AE(nn.Module):\n",
        "\tdef __init__(self, in_dims=784, hid_dims=100, negative_slope=0.1):\n",
        "\t\tsuper(AE, self).__init__()\n",
        "\t\t# Encoder\n",
        "\t\tself.encoder = nn.Sequential(OrderedDict([\n",
        "\t\t\t('layer1', nn.Linear(in_dims, 512)),\n",
        "\t\t\t('relu1', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "\t\t\t('layer2', nn.Linear(512, 256)),\n",
        "\t\t\t('relu2', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "\t\t\t('layer3', nn.Linear(256, 128)),\n",
        "\t\t\t('relu3', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "\t\t\t('layer4', nn.Linear(128, hid_dims)),\n",
        "\t\t\t('relu4', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "\t\t]))\n",
        "\t\t# Decoder\n",
        "\t\tself.decoder = nn.Sequential(OrderedDict([\n",
        "\t\t\t('layer1', nn.Linear(hid_dims, 128)),\n",
        "\t\t\t('relu1', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "\t\t\t('layer2', nn.Linear(128, 256)),\n",
        "\t\t\t('relu2', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "\t\t\t('layer3', nn.Linear(256, 512)),\n",
        "\t\t\t('relu3', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "\t\t\t('layer4', nn.Linear(512, in_dims)),\n",
        "\t\t\t('sigmoid', nn.Sigmoid()),\n",
        "\t\t]))\n",
        "\t\tself._init_weights()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tz = self.encoder(x)\n",
        "\t\ty = self.decoder(z)\n",
        "\t\treturn y\n",
        "\n",
        "\tdef _init_weights(self):\n",
        "\t\tfor m in self.modules():\n",
        "\t\t\tif isinstance(m, nn.Linear):\n",
        "\t\t\t\tm.weight.data.normal_(0, 0.01)\n",
        "\t\t\t\tm.bias.data.zero_()\n",
        "\t\t\telif isinstance(m, nn.BatchNorm2d):\n",
        "\t\t\t\tm.weight.data.fill_(1)\n",
        "\t\t\t\tm.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYdTmNJAOIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImproveChecker():\n",
        "\tdef __init__(self, mode='min', best_val=None):\n",
        "\t\tassert mode in ['min', 'max']\n",
        "\t\tself.mode = mode\n",
        "\t\tif best_val is not None:\n",
        "\t\t\tself.best_val = best_val\n",
        "\t\telse:\n",
        "\t\t\tif self.mode=='min':\n",
        "\t\t\t\tself.best_val = np.inf\n",
        "\t\t\telif self.mode=='max':\n",
        "\t\t\t\tself.best_val = 0.0\n",
        "\n",
        "\tdef check(self, val):\n",
        "\t\tif self.mode=='min':\n",
        "\t\t\tif val < self.best_val:\n",
        "\t\t\t\tprint(\"[%s] Improved from %.4f to %.4f\" % (self.__class__.__name__, self.best_val, val))\n",
        "\t\t\t\tself.best_val = val\n",
        "\t\t\t\treturn True\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"[%s] Not improved from %.4f\" % (self.__class__.__name__, self.best_val))\n",
        "\t\t\t\treturn False\n",
        "\t\telse:\n",
        "\t\t\tif val > self.best_val:\n",
        "\t\t\t\tprint(\"[%s] Improved from %.4f to %.4f\" % (self.__class__.__name__, self.best_val, val))\n",
        "\t\t\t\tself.best_val = val\n",
        "\t\t\t\treturn True\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"[%s] Not improved from %.4f\" % (self.__class__.__name__, self.best_val))\n",
        "\t\t\t\treturn False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tpp1zrfAQoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize AE\n",
        "model = AE(in_dims=784, hid_dims=100)\n",
        "model.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "dataset = datasets.MNIST(root='.', train=True, download=True,\n",
        "\ttransform=transforms.Compose([\n",
        "\t\ttransforms.ToTensor(),\n",
        "\t\ttransforms.Normalize((0.1307,), (0.3081,))\n",
        "]))\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "\tdataset, batch_size=128,\n",
        "\tnum_workers=4, shuffle=True, pin_memory=True,\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ImproveChecker\n",
        "improvechecker = ImproveChecker(mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWLyITDvAge0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()\n",
        "for epoch in range(1, 301):\n",
        "\tfor i, (imgs, _) in enumerate(dataloader):\n",
        "\t\t# Prepare input\n",
        "\t\tinputs = imgs.view(imgs.shape[0], -1)\n",
        "\t\tinputs = inputs.cuda()\n",
        "\n",
        "\t\t# Train\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\toutputs = model(inputs)\n",
        "\t\tloss = loss_fn(outputs, inputs)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t# ImproveChecker\n",
        "\tprint(\"[EPOCH %.3d] Loss: %.6f\" % (epoch, loss.item()))\n",
        "\tif improvechecker.check(loss.item()):\n",
        "\t\tcheckpoint = dict(\n",
        "\t\t\tepoch=epoch,\n",
        "\t\t\tloss=loss.item(),\n",
        "\t\t\tstate_dict=model.state_dict(),\n",
        "\t\t\toptimizer=optimizer.state_dict(),\n",
        "\t\t)\n",
        "\t\tsave_file = os.path.join('.', \"ae.pth\")\n",
        "\t\ttorch.save(checkpoint, save_file)\n",
        "\t\tprint(\"Best checkpoint is saved at %s\" % (save_file))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}